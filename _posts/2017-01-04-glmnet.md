`glmnet` is a R package for ridge regression, lasso regression, and elastic net. The authors of the package, Trevor Hastie and Junyang Qian, has written a beautiful vignette accompanying the package to demonstrate how to use the package: here is the [link to the version hosted on the homepage of T. Hastie](https://web.stanford.edu/~hastie/glmnet/glmnet_beta.html)(and [an ealier version written in 2014](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)).

Here are some background information one probably likes to know before using the package to solve any real-world problem. I summarized them from various resources; in case you have questions, critics, or comments, please do not hesitate to contact me.

## Ridge regression

Compared with ordinary linear regression with the least-square method, ridge regression is more suited when some variables are correlated (collinearity). When collinearity exists, the least-square method will lead to huge variance of the estimates. Because ridge regression adds bias to the estimates, it can reduce the variance of the estimates, due to the so-called [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).

In essence, ridge regression applies a regularization which can be tuned by a non-negative parameter `gamma`: when `gamma=0`, no regularization is applied and the solution becomes identical with that of the least-square solution; otherwise the regularization is applied, and consequently large regression coefficients are shrinked to reduce overfitting. See more details on the Wikipedia page about [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization), also known as [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization).

## Lasso regression

LASSO, or __l__east __a__boslute __s__hrinkage and __s__election __o__perator, is another regression method that performs both variable selection and regularization. It selects only a subset of provided covariates in the final model rather than using all of them for regression. As a consequence, the resulting model has fewer variables and is easiert to interpret, while maintaining a high accuracy of prediction. 

Technically, LASSO achieves its goals by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value, and therefore forces certain coefficients be set to zero. This process is also known as ___l___1 normalization, in contrast to the ___l___2 normalization applied in ridge regression, which forces the sum of __the squares of__ the coefficients to be less than a fixed value.

One limitation of LASSO when applied to computational biology questions is that when the number of variables is larger than the same size (___p>n___), lasso can only select ___n___ covariates, even when more are associated with the outcome, and it tends to select only one covariate from any set of highly correlated covariates. 

To understand this limitation in a real setting, imagine a prediction problem using gene expression values to predict toxicological outcome of a sample. In case a group of genes that are highly correlated with each other, for instance genes involved in DNA damage and repair, are associated with the outcome, using LASSO will probably select only one gene from them. While this may be fine from a pure prediction point of view, the result of feature selection can be misleading if one wants to test which biological functions are enriched in the genes that are associated with the outcome, because while many genes associated with DNA damage and repair are associated, only one of them will be reported by LASSO.

## Elastic net

Elastic net can be viewed as a hybrid of ridge and lasso. It extends LASSO by adding a ridge regression-like penalty which improves performance when ___p>n___. The result is that strongly correlated variables can be selected together.

The elastic-net penalty is controlled by $alpha$, and bridges the gap between lasso ($alpha$=1) and ridge ($alpha$=0).

From the discussions above, we understand that ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an $alpha$=0.5 tends to select the groups in or out together. Another use of $alpha$ is for numerical stability; for example, the elastic net with $alpha$=1âˆ’$epsilon$ for some small $epsilon$>0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations.
